# 環境設定
env_id: UTTTRLSim-v0
# 推奨値: 変更不可
# 説明: 環境の識別子
# 決め方: 環境実装時に固定

env_wrapper: "random"
# 推奨値: "none", "random"
# 説明: 環境ラッパーの種類
# 決め方: "none" = 自己対戦, "random" = RandomAgent対戦

n_envs: 16
# 推奨値: 4-16
# 説明: 並列で実行する環境の数
# 決め方: 多いほど学習が安定するが計算コストが増加

total_steps: 10000
# 推奨値: 10000-100000
# 説明: 総学習ステップ数
# 決め方: タスクの複雑さに応じて調整

seed: 42
# 推奨値: 固定値
# 説明: 乱数シード
# 決め方: 再現性のため固定推奨

# PPOアルゴリズムのハイパーパラメータ
ppo_params:
  learning_rate: 0.0003
  # 推奨値: 0.0001-0.001
  # 説明: 学習率
  # 決め方: 大きすぎると発散、小さすぎると学習が遅い

  batch_size: 256
  # 推奨値: 64-256
  # 説明: バッチサイズ
  # 決め方: 大きいほど安定だがメモリ使用量増加

  n_steps: 128
  # 推奨値: 64-2048
  # 説明: 各更新前のステップ数
  # 決め方: 長いほど高品質なデータだが更新頻度が下がる

  ent_coef: 0.01
  # 推奨値: 0.001-0.1
  # 説明: エントロピー係数
  # 決め方: 探索を促進、大きいほどランダムな行動

  vf_coef: 0.5
  # 推奨値: 0.25-1.0
  # 説明: 価値関数係数
  # 決め方: 価値関数学習の重み

  max_grad_norm: 0.5
  # 推奨値: 0.1-1.0
  # 説明: 勾配クリッピング
  # 決め方: 勾配爆発を防ぐ

  gamma: 0.99
  # 推奨値: 0.9-0.999
  # 説明: 割引率
  # 決め方: 将来の報酬の重要度

  gae_lambda: 0.95
  # 推奨値: 0.9-0.99
  # 説明: GAEラムダ
  # 決め方: バイアスと分散のトレードオフ

  clip_range: 0.2
  # 推奨値: 0.1-0.3
  # 説明: PPOクリップ範囲
  # 決め方: 大きいほど大きな更新を許可

  policy_kwargs:
    net_arch: [ 64, 64 ]
    # 推奨値: [64,64] または [128,128]
    # 説明: ネットワークアーキテクチャ
    # 決め方: タスクの複雑さに応じて調整

    # Random対戦版専用設定
random_training:
  opponent_seed: 42
  # 推奨値: 固定値
  # 説明: RandomAgentの乱数シード
  # 決め方: 再現性のため固定推奨

  # Random対戦版用のPPOハイパーパラメータ（基本設定を上書き）
  ppo_params:
    learning_rate: 0.0001
    # Random対戦版ではより小さな学習率
    batch_size: 512
    # Random対戦版ではより大きなバッチサイズ
    ent_coef: 0.005
    # Random対戦版ではより小さなエントロピー係数

    # ログとモデル保存の設定
tensorboard_log: logs/tb
# 推奨値: logs/tb
# 説明: TensorBoardログディレクトリ
# 決め方: 学習の可視化用

model_path: models/policy.zip
# 推奨値: models/policy.zip
# 説明: モデル保存パス
# 決め方: 学習済みモデルの保存先

log_interval: 1

# 推奨値: 1-10
# 説明: ログ間隔
# 決め方: 頻繁すぎるとログが肥大化
